selected_columns
# Chunk 5
# keep only the rows that have a value in the column "OfferID" and/or "Selected" AND have the value "O_Create Offer" or "O_Created" in the column "Activity"
filtered_data <- selected_columns %>% dplyr::filter((!is.na(OfferID) | !is.na(Selected)) & (Activity == "O_Create Offer" | Activity == "O_Created"))
filtered_data
# Chunk 6
# copy the value in OfferID of each row in the row that comes before it
modified_data <- filtered_data %>%
fill(OfferID, .direction = "up")
# then delete the column that originally contained OfferID and count the number of non-zero values in "CreditScore" (to later check)
final_data <- modified_data %>%
filter(!is.na(Selected))
final_data
sum(final_data$CreditScore != 0, na.rm = TRUE)
# Chunk 7
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
# I am keeping both dataframes with 0 and median value (>0) because I'm not sure of what choice is theoretically correct
library(conflicted)
conflicts_prefer(dplyr::select())
knitr::opts_chunk$set(warning=FALSE,
message=FALSE,
tidy.opts=list(width.cutoff = 60),
tidy = TRUE)
knitr::opts_knit$set(global.par=TRUE)
# built-in output hook
hook_output <- knitr::knit_hooks$get("output")
# custom chunk option output.lines to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
n <- options$output.lines
if (!is.null(n)) {
x <- xfun::split_lines(x)
if(length(x) > n) {
top <- head(x, n)
bot <- tail(x, n)
x <- c(top, "\n....\n", bot)
}
x <- paste(x, collapse="\n")
}
hook_output(x, options)
})
library(tidyverse)
###### Change the wd based on your enviroment
setwd("C:/Users/gianl/Desktop/0_TUe materials/AssignmentPM/adv-pm/r")
data <- read_csv("BPI_Challenge_2017_filtered.csv")
data
# keep only interesting columns: Identification (Case ID, Activity, Start Timestamp, Variant index) Features (NumberOfTerms, OfferedAmount, MonthlyCost) and Target variable (Selected)
selected_columns <- data %>% select("Case ID", "Activity", "Start Timestamp", "Variant index", "NumberOfTerms", "OfferedAmount", "MonthlyCost", "CreditScore", "OfferID", "Selected")
selected_columns
# keep only the rows that have a value in the column "OfferID" and/or "Selected" AND have the value "O_Create Offer" or "O_Created" in the column "Activity"
filtered_data <- selected_columns %>% dplyr::filter((!is.na(OfferID) | !is.na(Selected)) & (Activity == "O_Create Offer" | Activity == "O_Created"))
filtered_data
# copy the value in OfferID of each row in the row that comes before it
modified_data <- filtered_data %>%
fill(OfferID, .direction = "up")
# then delete the column that originally contained OfferID and count the number of non-zero values in "CreditScore" (to later check)
final_data <- modified_data %>%
filter(!is.na(Selected))
# copy the value in OfferID of each row in the row that comes before it
modified_data <- filtered_data %>%
fill(OfferID, .direction = "up")
# then delete the column that originally contained OfferID and count the number of non-zero values in "CreditScore" (to later check)
final_data <- modified_data %>%
dplyr::filter(!is.na(Selected))
final_data
sum(final_data$CreditScore != 0, na.rm = TRUE)
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
# I am keeping both dataframes with 0 and median value (>0) because I'm not sure of what choice is theoretically correct
# Chunk 1: setup
library(conflicted)
conflicts_prefer(dplyr::select())
knitr::opts_chunk$set(warning=FALSE,
message=FALSE,
tidy.opts=list(width.cutoff = 60),
tidy = TRUE)
knitr::opts_knit$set(global.par=TRUE)
# built-in output hook
hook_output <- knitr::knit_hooks$get("output")
# custom chunk option output.lines to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
n <- options$output.lines
if (!is.null(n)) {
x <- xfun::split_lines(x)
if(length(x) > n) {
top <- head(x, n)
bot <- tail(x, n)
x <- c(top, "\n....\n", bot)
}
x <- paste(x, collapse="\n")
}
hook_output(x, options)
})
# Chunk 2
library(tidyverse)
library(ISLR2)
library(glmnet)
dataf <- na.omit(Hitters)
# Chunk 3
x <- model.matrix(Salary ~ ., data=dataf)[, -1] # trim off the 1st column (i.e., keep only the predictors)
y <- dataf$Salary
# Chunk 4
grid <- 10^seq(10, -2, length=100)
ridge_mod <- glmnet(x, y, alpha=0, lambda=grid)
# Chunk 5
dim(coef(ridge_mod))
plot(ridge_mod) # plot the coefficients vs their L1 norm
plot(ridge_mod, xvar="lambda") # coefficients vs log lambda
# Chunk 6
ridge_mod$lambda[50] # display the 50th lambda value
log(ridge_mod$lambda[50])
coef(ridge_mod)[, 50] # display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1, 50]^2)) # calculate l2 norm
ridge_mod$lambda[50] # display the 50th lambda value
log(ridge_mod$lambda[50])
coef(ridge_mod)[, 50] # display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1, 50]^2)) # calculate l2 norm
ridge_mod$lambda[60] # display the 60th lambda value
log(ridge_mod$lambda[60])
coef(ridge_mod)[, 60] # display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge_mod)[-1, 60]^2)) # Calculate l2 norm
predict(ridge_mod, s=50, type="coefficients")[1:20, ]
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- -train # numerical indexes, so we complement with -
y_test <- y[test]
x_train <- model.matrix(Salary ~ ., dataf[train, ])[, -1]
x_test <- model.matrix(Salary ~ ., dataf[-train, ])[, -1]
# or:
# x_train <- x[train, ]
# x_test <- x[test, ]
y_train <- y[train]
ridge_mod <- glmnet(x_train, y_train, alpha=0) # NOT manually specifying a grid this time:
ridge_pred <- predict(ridge_mod, s=4, newx=x_test)
mean((ridge_pred - y_test)^2)
mean((mean(y_train) - y_test)^2)
ridge_pred <- predict(ridge_mod, s=1e10, newx=x_test)
mean((ridge_pred - y_test)^2)
compute_mse <- function(preds, truth) {
mean((preds - truth)^2)
}
ridge_pred <- predict(ridge_mod, s=0, newx=x_test, exact=TRUE, x=x_train, y=y_train)
compute_mse(ridge_pred, y_test)
lm(Salary ~ ., data=dataf, subset=train)
predict(ridge_mod, s=0, exact=TRUE, type="coefficients", x=x_train, y=y_train)[1:20, ]
set.seed(1)
# fit ridge regression model on training data, using the lambda grid from the previously fitted model
cv.out <- cv.glmnet(x_train, y_train, alpha=0, lambda=ridge_mod$lambda)
# select lambda that minimizes training MSE
bestlam <- cv.out$lambda.min
bestlam
# draw plot of training MSE as a function of lambda
plot(cv.out)
ridge_pred <- predict(ridge_mod, s=bestlam, newx=x_test) # use best lambda to predict on test data
compute_mse(ridge_pred, y_test) # calculate test MSE
out <- glmnet(x, y, alpha=0) # fit ridge regression model on full dataset
predict(out, type="coefficients", s=bestlam)[1:20, ] # display coefficients using lambda chosen by CV
final_data %>%
select(Column1, Column2, Column3, ...) %>%
cor(selected_columns)
final_data %>%
select(NumberOfTerms, OfferedAmount, MonthlyCost, CreditScore, Selected) %>%
cor()
cor_matrix <- final_data %>%
select(NumberOfTerms, OfferedAmount, MonthlyCost, CreditScore, Selected) %>%
cor()
corr_data
cor_matrix <- final_data %>%
select(NumberOfTerms, OfferedAmount, MonthlyCost, CreditScore, Selected) %>%
cor()
cor_matrix
library(corrplot)
# Customize the correlation plot if needed
corrplot(cor_matrix, method = "color")
{r}
cor_matrix <- final_data_median %>%
select(NumberOfTerms, OfferedAmount, MonthlyCost, CreditScore, Selected) %>%
cor()
cor_matrix
library(corrplot)
# Customize the correlation plot if needed
corrplot(cor_matrix, method = "color")
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
# I am keeping both dataframes with 0 and median value (>0) because I'm not sure of what choice is theoretically correct
final_data_erase <- final_data %>%
dplyr::filter(CreditScore == 0)
final_data_erase <- final_data %>%
dplyr::filter(CreditScore != 0)
final_data_erase <- final_data %>%
dplyr::filter(CreditScore != 0)
sum(final_data_erase$CreditScore != 0, na.rm = TRUE)
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
final_data_erase <- final_data %>%
dplyr::filter(CreditScore != 0)
sum(final_data_erase$CreditScore != 0, na.rm = TRUE)
final_data %>%
dplyr::filter(CreditScore != 0)
final_data_erase <- final_data %>%
dplyr::filter(CreditScore == 0)
sum(final_data_erase$CreditScore != 0, na.rm = TRUE)
final_data_erase
final_data
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
# I am keeping both dataframes with 0 and median value (>0) because I'm not sure of what choice is theoretically correct
final_data_erase <- final_data %>%
dplyr::filter(CreditScore != 0)
sum(final_data_erase$CreditScore != 0, na.rm = TRUE)
# copy the value in OfferID of each row in the row that comes before it
modified_data <- filtered_data %>%
fill(OfferID, .direction = "up")
# then delete the column that originally contained OfferID and count the number of non-zero values in "CreditScore" (to later check)
final_data <- modified_data %>%
dplyr::filter(!is.na(Selected))
final_data
sum(final_data$CreditScore != 0, na.rm = TRUE)
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data_zero <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data_zero
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
# I am keeping both dataframes with 0 and median value (>0) because I'm not sure of what choice is theoretically correct
final_data_erase <- final_data %>%
dplyr::filter(CreditScore != 0)
sum(final_data_erase$CreditScore != 0, na.rm = TRUE)
#  for each instance in which more than one row has the same "Case ID" I want to check if there is a non-missing value bigger than 0 in the column "CreditScore" for each row with the same "Case ID". If there is at least one, I want to copy it and paste it in the other rows with the same "Case ID" that have a missing value or 0-value in the column "CreditScore"
final_data_zero <- final_data %>%
group_by(`Case ID`) %>%
mutate(
CreditScore = ifelse(any(CreditScore > 0 & !is.na(CreditScore)), max(CreditScore[CreditScore > 0]), CreditScore)
) %>%
ungroup()
final_data_zero
# check if changes were made (yesss)
sum(final_data$CreditScore != 0, na.rm = TRUE)
final_data_median <- final_data %>%
mutate(CreditScore = ifelse(CreditScore == 0 | is.na(CreditScore), median(CreditScore[CreditScore != 0], na.rm = TRUE), CreditScore))
final_data_median
# I am keeping both dataframes with 0 and median value (>0) because I'm not sure of what choice is theoretically correct
final_data_erase <- final_data %>%
dplyr::filter(CreditScore != 0)
final_data_erase
sum(final_data_erase$CreditScore != 0, na.rm = TRUE)
cor_matrix <- final_data %>%
select(NumberOfTerms, OfferedAmount, MonthlyCost, Selected) %>%
cor()
cor_matrix
library(corrplot)
# Customize the correlation plot if needed
corrplot(cor_matrix, method = "color")
pairs(dataf)
# Chunk 1: setup
knitr::opts_chunk$set(
warning = FALSE,
message = FALSE,
tidy.opts = list(width.cutoff = 60),
tidy = TRUE
)
# Chunk 2
library(tidyverse)
library(ISLR2)
# View(Smarket)
# Chunk 3
head(Smarket)
# Chunk 4
as_tibble(Smarket)
# Chunk 5
names(Smarket)
dim(Smarket)
summary(Smarket)
# Chunk 6
dataf <- Smarket
pairs(dataf)
knitr::opts_chunk$set(
warning = FALSE,
message = FALSE,
tidy.opts = list(width.cutoff = 60),
tidy = TRUE
)
library(tidyverse)
library(ISLR2)
# View(Smarket)
head(Smarket)
as_tibble(Smarket)
names(Smarket)
dim(Smarket)
summary(Smarket)
dataf <- Smarket
pairs(dataf)
cor(dataf)
cor(dplyr::select(dataf, -Direction))
attach(final_data)
glm.fits <- glm(Selected ~ NumberOfTerms + OfferedAmount + MonthlyCost + CreditScore,
data = final_data,
family = binomial
) # family=binomial selects a Logistic Regression model
glm.fits
summary(glm.fits)
attach(final_data)
glm.fits <- glm(Selected ~ NumberOfTerms + OfferedAmount + MonthlyCost + CreditScore,
data = final_data,
family = binomial
) # family=binomial selects a Logistic Regression model
glm.fits
summary(glm.fits)
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
glm.probs <- predict(glm.fits, type = "response")
glm.probs
glm.probs <- predict(glm.fits, type = "response")
glm.probs
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:50]
contrasts(Selected)
knitr::opts_chunk$set(
warning = FALSE,
message = FALSE,
tidy.opts = list(width.cutoff = 60),
tidy = TRUE
)
library(tidyverse)
library(ISLR2)
# View(Smarket)
head(Smarket)
as_tibble(Smarket)
names(Smarket)
dim(Smarket)
summary(Smarket)
dataf <- Smarket
pairs(dataf)
cor(dataf[, -9])
cor(dplyr::select(dataf, -Direction))
attach(dataf)
print(head(Volume, n = 10))
plot(Volume)
plot(Year, Volume)
Volume_means <- aggregate(Volume, list(Year), mean)
Volume_means
plot(Volume_means)
ggplot(dataf, aes(Year, Volume)) +
geom_jitter(height = 0)
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = dataf,
family = binomial
) # family=binomial selects a Logistic Regression model
glm.fits
summary(glm.fits)
library(tidymodels)
lr_spec <- logistic_reg() %>% # define a generalized linear model for binary outcomes
set_engine("glm") %>% # declare which package will be used to fit the model
set_mode("classification") # set model's mode to classification
lr_fit <- lr_spec %>%
fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = dataf
)
lr_fit
lr_fit %>%
pluck("fit") %>% # this function from the purrr library selects the "fit" slot
summary()
lr_stats <- tidy(lr_fit)
lr_stats
lr_stats$estimate
glance(lr_fit)
# accesses only coefficients
coef(glm.fits)
# same as coef(glm.fits)
summary(glm.fits)$coef[, 1]
# same as tidy(lr_fit)
summary(glm.fits)$coef
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
contrasts(Direction)
glm.pred <- rep("Down", nrow(dataf)) # create a "placeholder" filled with as many Down values as the number of observations (rows) in dataf
glm.pred[glm.probs > 0.5] <- "Up" # replace with Up values according to the glm.probs threshold
predict(lr_fit, new_data = dataf, type = "prob")
contrasts(Selected)
glm.pred <- rep("Down", nrow(final_data)) # create a "placeholder" filled with as many Down values as the number of observations (rows) in dataf
glm.pred[glm.probs > 0.5] <- "Up" # replace with Up values according to the glm.probs threshold
predict(glm.fits, new_data = final_data, type = "prob")
library(tidymodels)
lr_spec <- logistic_reg() %>% # define a generalized linear model for binary outcomes
set_engine("glm") %>% # declare which package will be used to fit the model
set_mode("classification") # set model's mode to classification
library(tidymodels)
lr_spec <- logistic_reg() %>% # define a generalized linear model for binary outcomes
set_engine("glm") %>% # declare which package will be used to fit the model
set_mode("classification") # set model's mode to classification
lr_fit <- lr_spec %>%
fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = dataf
)
lr_fit
library(tidymodels)
lr_spec <- logistic_reg() %>% # define a generalized linear model for binary outcomes
set_engine("glm") %>% # declare which package will be used to fit the model
set_mode("classification") # set model's mode to classification
lr_fit <- lr_spec %>%
fit(Selected ~ NumberOfTerms + OfferedAmount + MonthlyCost + CreditScore,
data = final_data,
)
library(tidymodels)
lr_spec <- logistic_reg() %>% # define a generalized linear model for binary outcomes
set_engine("glm") %>% # declare which package will be used to fit the model
set_mode("classification") # set model's mode to classification
lr_fit <- lr_spec %>%
fit(as.factor(Selected) ~ NumberOfTerms + OfferedAmount + MonthlyCost + CreditScore,
data = final_data,
)
lr_fit
lr_stats <- tidy(lr_fit)
lr_stats
predict(glm.fits, new_data = final_data, type = "prob")
predict(lr_fit, new_data = final_data, type = "prob")
predict(lr_fit, new_data = dataf, type = "class")
predict(lr_fit, new_data = final_data, type = "prob")
predict(lr_fit, new_data = dataf, type = "class")
predict(lr_fit, new_data = final_data, type = "prob")
predict(lr_fit, new_data = dataf, type = "class")
# confusion matrix
table(glm.pred, Direction) # predictions vs truth
predict(lr_fit, new_data = final_data, type = "prob")
predict(lr_fit, new_data = dataf, type = "class")
# confusion matrix
table(glm.pred, Selected) # predictions vs truth
# correct predictions / total observations
# "by hand" (the horror...)
(507 + 145) / 1250
# equivalent but more elegant
mean(glm.pred == Selected)
# and of course we could also compute the error ("!=" means "not equal to")
mean(glm.pred != Selected)
predict(lr_fit, new_data = final_data, type = "prob")
predict(lr_fit, new_data = dataf, type = "class")
# confusion matrix
table(glm.pred, Selected) # predictions vs truth
# correct predictions / total observations
# "by hand" (the horror...)
#(507 + 145) / 1250
# equivalent but more elegant
mean(glm.pred == Selected)
# and of course we could also compute the error ("!=" means "not equal to")
mean(glm.pred != Selected)
# equivalent but more elegant
mean(glm.pred == Selected)
glm.pred == Selected
mean(glm.pred != Selected)
predict(lr_fit, new_data = final_data, type = "prob")
